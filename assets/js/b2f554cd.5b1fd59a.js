"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2024/06/01/Post1","metadata":{"permalink":"/blog/2024/06/01/Post1","source":"@site/blog/2024-06-01/Post1.md","title":"The Zalgo Effect and Resource Leakage - A Case","description":"Zalgo Effect is an term used to describe unexpected outcomes of mixing sync and async javascript code.","date":"2024-06-01T00:00:00.000Z","formattedDate":"June 1, 2024","tags":[{"label":"nodejs","permalink":"/blog/tags/nodejs"},{"label":"zalgoeffect","permalink":"/blog/tags/zalgoeffect"},{"label":"code","permalink":"/blog/tags/code"}],"readingTime":3.08,"hasTruncateMarker":false,"authors":[{"name":"Lucas Weis Polesello","title":"SRE | Senior Software Engineer @ LumaHealth","url":"https://github.com/lukas8219","email":"lucas.polesello@lwpsoftwares.com | lucas.c4d@gmail.com","imageURL":"https://github.com/lukas8219.png","key":"lukas8219"}],"frontMatter":{"title":"The Zalgo Effect and Resource Leakage - A Case","authors":"lukas8219","tags":["nodejs","zalgoeffect","code"]},"nextItem":{"title":"Redis is more than a Cache-n1 - Delaying Jobs","permalink":"/blog/2024/06/01/Post2"}},"content":"Zalgo Effect is an term used to describe unexpected outcomes of mixing `sync` and `async` javascript code.\\n\\nIt means - if you mix these two approaches SOMETHING weird will happen.\\n\\nIt\'s one of those things you kinda _don\'t understand_ until you see it in real production systems.\\n\\n# So what it has to do with Resource Leakage?\\n\\nOne day, our SRE team received a couple PagerDuty alerts claiming our services were restarting and not able to work properly due to `Error: No channels left to allocate` - ie RabbitMQ connections were maxing out in channel allocation. (For RabbitMQ reference into Channels and Connections)\\n\\nIt was clear some code was leaking channel creations. No one knew what could potentially be - but God I had studied this `Zalgo Effect` in `NodeJS Design Patterns` Book and it clicked me something.\\n\\n# How was I so sure the Zalgo was the culprit?\\nThe service that was throwing that error was only responsible for fan out a couple messages to a lot of other services - so it was easy as creating a `Queue` object and running N promises concurrently to publish some message.\\nChecking the RabbitMQ Management UI showed me that we created N channels for that connection.\\n\\n# But why it only happened in some scenarios?\\n\\nThat\'s where the `Zalgo Effect` pops in.\\n\\nOur code was built back in ~2015 - Node 4. The callback style was the mainstream. Our Engineers created the abstraction `Queue` which dealt with almost 50% of our Event-Driven Architecture by itself and had to make the `class` style w/ async initializations - not so easily with callbacks.\\n\\nSo the code assumed the following:\\n1. Assert exchange, queues and necessary resources - via something we could call `consumeChannel`.\\n\\t1. The consume channel is created whenever the connection is made.\\n2. Our `confirmChannel` - ie the channel we used to `publish` events was lazily created - mixing `async` and `sync` code.\\n\\nSo the problem lives in 2).\\n\\nImagine the following:\\n\\n- We `assertConfirmChannel`\\n[Download Captura_de_Tela_2024-03-15_\xe0s_19.06](/assets/Captura_de_Tela_2024-03-15_\xe0s_19.06)\\n\\n[Download Captura_de_Tela_2024-03-15_\xe0s_19.07](/assets/Captura_de_Tela_2024-03-15_\xe0s_19.07)\\n\\n[Download Captura_de_Tela_2024-03-15_\xe0s_19.07](/assets/Captura_de_Tela_2024-03-15_\xe0s_19.07)\\n- It check\'s whether the channel EXISTS or NOT.\\n- If not, create via `PROMISE` and return control to EventLoop\\n- If does, return it\\n\\nWhat happens, if the `two concurrent promises` reaches the same `if` without the first promise resolving? We try to create the channel two times and override them - thus keeping channels open but just using only the last one.\\n\\nThis is where the code was _leaking_ channels.\\n\\n## Fixing the problem\\n\\nWell, the fix we _actually shipped_ was simply calling 1 Promise and await it and then fan out the other promises.\\n\\nBut we made it simple due to risks and since the code is being refactored into a new style.\\n\\n# How can I fix If I see something like that?\\n\\nIf you want a real solution, here\'s what the V2 would look like - the idea is to create Promises and assign variables with them, instead of doing `await` on it. Example as below:\\n\\n[Download Captura_de_Tela_2024-03-15_\xe0s_19.18](/assets/Captura_de_Tela_2024-03-15_\xe0s_19.18)\\n\\nThis easily fixes the problem - by setting a variable as promise and checking its existence.\\n\\nA more robust style, where you actually need to initialize a couple of resources, you could do something like below\\n\\n[Download Captura_de_Tela_2024-03-15_\xe0s_19.19](/assets/Captura_de_Tela_2024-03-15_\xe0s_19.19)\\n\\n1. Create a function to execute the entire Promise.\\n2. Set up some reference to it\\n3. If requested the same, just use the same Promise.\\n\\n\\n# Ok - but why it fixes the problem?\\n\\nThe idea is to make sure - we are running things in a sync manner and just making the promises settled on their timing. We need to think about the synchronous code execution block to reason about our promises usage."},{"id":"/2024/06/01/Post2","metadata":{"permalink":"/blog/2024/06/01/Post2","source":"@site/blog/2024-06-01/Post2.md","title":"Redis is more than a Cache-n1 - Delaying Jobs","description":"My current company - Luma Health Inc - has an Event-Driven Architecture where all of our backend systems interact via async messaging/jobs. Thus our backbone is sustained by an AMQP broker - RabbitMQ - which routes the jobs to interested services.","date":"2024-06-01T00:00:00.000Z","formattedDate":"June 1, 2024","tags":[{"label":"redis","permalink":"/blog/tags/redis"},{"label":"jobs","permalink":"/blog/tags/jobs"},{"label":"pubsub","permalink":"/blog/tags/pubsub"},{"label":"luma","permalink":"/blog/tags/luma"},{"label":"more","permalink":"/blog/tags/more"},{"label":"than","permalink":"/blog/tags/than"},{"label":"cache","permalink":"/blog/tags/cache"}],"readingTime":3.295,"hasTruncateMarker":false,"authors":[{"name":"Lucas Weis Polesello","title":"SRE | Senior Software Engineer @ LumaHealth","url":"https://github.com/lukas8219","email":"lucas.polesello@lwpsoftwares.com | lucas.c4d@gmail.com","imageURL":"https://github.com/lukas8219.png","key":"lukas8219"}],"frontMatter":{"title":"Redis is more than a Cache-n1 - Delaying Jobs","authors":"lukas8219","tags":["redis","jobs","pubsub","luma","more","than","cache"]},"prevItem":{"title":"The Zalgo Effect and Resource Leakage - A Case","permalink":"/blog/2024/06/01/Post1"},"nextItem":{"title":"NodeJS Lazy Initialization","permalink":"/blog/2024/06/01/Post3"}},"content":"My current company - [Luma Health Inc](https://www.lumahealth.io/) - has an `Event-Driven Architecture` where all of our backend systems interact via async messaging/jobs. Thus our backbone is sustained by an AMQP broker - RabbitMQ - which routes the jobs to interested services.\\n\\nSince our jobs are very critical - we cannot support failures AND should design to make the system more resilient because well..we don\'t want a patient not being notified of their appointment, appointments not being created when they should, patients showing off into facilities where they were never notified the patient had something scheduled.\\n\\nBesides the infra and product reliability - some use cases could need postponing - maybe reaching out to an external system who\'s offline/or not responding. Maybe some error which needs a retry - who knows? \\n\\nThe fact is, delaying/retry is a very frequent requirement into Event Driven Architectures. With this a service responsible for doing it was created - and it worked fine.\\n\\nBut - as the company sold bigger contracts and grew up in scale - this system was almost stressed out and not reliable.\\n\\n\\n## The Unreliable Design\\n\\nBefore giving the symptoms, let\'s talk about the organism itself - the service old design.\\n\\nThe design was really straightforward - if our service handlers asked for a postpone OR we failed to send the message to RabbitMQ - we just insert the JSON object from the Job into a Redis `Sorted Set` and using the `Score` as the timestamp which it was meant to be retried/published again.\\n\\nTo publish back into RabbitMQ the postponed messages, a job would be triggered each 5 seconds - doing the following:\\n1. Read from a `set` key containing all the existing `sorted set` keys - basically the queue name\\n2. Fetch run a `zrangebyscore` from 0 to current timestamp BUT `limit` to 5K jobs.\\n3. Publish the job and remove it from `sorted set`\\n\\n## The Issues\\n\\nThis solution actually scaled up until 1-2 years ago when we started having issues with it - the main one\'s being:\\n1. It could not catch up to a huge backlog of delayed messages\\n2. It would eventually OOM or SPIKE up to 40GB of memory\\n\\t1. Due to things being fetched into memory AND some instability OR even some internal logic - we could end up shoveling too much data into Redis - the service just died :skull:\\n3. We could not scale horizontally - due to consuming and fetching objects into memory before deleting them.\\n\\n## The solution\\n\\nThe solution was very simple: we implemented something that I liked to call `streaming approach`\\n\\nUsing the same data structure, we are now:\\n1. Running a `zcount` from 0 to current timestamp\\n\\t- Counting the amount of Jobs -> returning N\\n1. Creating an `Async Iterator` for N times - that used the `zpopmin` method from Redis\\n\\t- `zpopmin` basically returns AND removes the least score object - ie most recent timestamp\\n\\nThe `processor` for the SortedSet\\n[Download Captura_de_Tela_2024-03-15_\xe0s_19.47](/assets/Captura_de_Tela_2024-03-15_\xe0s_19.47)\\n\\nThe `Async Iterator`\\n[Download Captura_de_Tela_2024-03-15_\xe0s_19.47](/assets/Captura_de_Tela_2024-03-15_\xe0s_19.47)\\n\\nAnd that\'s _all!\\n\\nThis simple algorithm change annihilated the need for:\\n1. Big In Memory fetches - makes our memory allocation big\\n2. Limit of 5K in fetches - makes our throughput lower\\n\\n## Results\\n\\nI think the screenshots can speak for themselves but:\\n- We processed the entire backlog of 40GB of pending jobs pretty quickly\\n- From a constant usage of ~8GB - we dropped down to ~200MB\\n- We are now - trying to be play safe and still oversize - safely allocating 1/4 of the resources.\\n\\nMoney-wise: We are talking at least of 1K USD/month AND more in the future if we can lower our Rediscache instance.\\n\\n[Download Captura_de_Tela_2024-03-14_\xe0s_22.45](/assets/Captura_de_Tela_2024-03-14_\xe0s_22.45)\\n\\n[Download Captura_de_Tela_2024-03-14_\xe0s_23.52](/assets/Captura_de_Tela_2024-03-14_\xe0s_23.52)\\n[Download Captura_de_Tela_2024-03-15_\xe0s_20.05](/assets/Captura_de_Tela_2024-03-15_\xe0s_20.05)\\n\\n### Note\\nWe currently have more enhancements in the roadmap - such as making the job delaying via RPC, using different storages for different postpone amount (1milli, 1 second, 1 day, 1 week++) and making it more reliable overall."},{"id":"/2024/06/01/Post3","metadata":{"permalink":"/blog/2024/06/01/Post3","source":"@site/blog/2024-06-01/Post3.md","title":"NodeJS Lazy Initialization","description":"One of the main challenges when dealing w/ the async nature of NodeJS is initializing classes/clients that requires some sort of side effect - such as database connection, disk reads or whatsoever. Even the simple idea of waiting for the first use-case to connect/initialize a resource.","date":"2024-06-01T00:00:00.000Z","formattedDate":"June 1, 2024","tags":[{"label":"nodejs","permalink":"/blog/tags/nodejs"},{"label":"lazyinitialization","permalink":"/blog/tags/lazyinitialization"}],"readingTime":1.88,"hasTruncateMarker":false,"authors":[{"name":"Lucas Weis Polesello","title":"SRE | Senior Software Engineer @ LumaHealth","url":"https://github.com/lukas8219","email":"lucas.polesello@lwpsoftwares.com | lucas.c4d@gmail.com","imageURL":"https://github.com/lukas8219.png","key":"lukas8219"}],"frontMatter":{"title":"NodeJS Lazy Initialization","authors":"lukas8219","tags":["nodejs","lazyinitialization"]},"prevItem":{"title":"Redis is more than a Cache-n1 - Delaying Jobs","permalink":"/blog/2024/06/01/Post2"},"nextItem":{"title":"Redis is more than a Cache 2 -> Reference Data","permalink":"/blog/2024/06/01/Post4"}},"content":"One of the main challenges when dealing w/ the async nature of NodeJS is initializing classes/clients that requires some sort of side effect - such as database connection, disk reads or whatsoever. Even the simple idea of waiting for the first use-case to connect/initialize a resource.\\n\\nBesides Dependency Injection - I like to use two approaches for this:\\n\\n1) Leaving it up to the client to call `connect` or any other synonym - easy as creating an `async function` as the example below\\n```javascript\\nconst redis = require(\'redis\');\\nconst crypto = require(\'crypto\');\\n//PROS: Damn easy, simple and straight-forward\\n\\n//CONS: This leaves the entire responsibility to the client\\nclass DistributedDataStructure {\\n\\tconstructor(){\\n\\t\\tthis.client = redis.createClient();\\n\\t}\\n\\n\\tasync connect(){\\n\\t\\treturn this.client.connect();\\n\\t}\\n\\n\\tasync add(staffName, reviewId){\\n\\t\\t//Do some business here - idk,\\n\\t\\tconst accountName = await this.client.get(key);\\n\\t\\treturn this.client.sAdd(`v1:${accountName}:pending-reviews`, reviewId);\\n\\t}\\n}\\n\\n(async () => {\\n\\tconst ds = new DistributedDataStructure();\\n\\tawait ds.connect();\\n\\tds.add(\'Jerome\', crypto.randomBytes(12).toString(\'hex\'));\\n})()\\n```\\n\\n\\n2) Proxying the access\\n\\nIn the real and wild-world we know that we have to deal w/ legacy code, legacy initialization methods and much more unexpected stuff - for this we have a second use-case which leverages the (https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy])[Proxy API for JS]\\n\\nUsing Proxy it would look poorly-like\\n\\n```javascript\\nconst redis = require(\'redis\');\\nconst { once } = require(\'events\');\\nconst crypto = require(\'crypto\');\\n\\n//PROS: No client responsibility - makes it easy for the client\\n//CONS: More complex and error prone\\nclass ProxiedDistributedDataStructure {\\n\\tconstructor(){\\n\\t\\tthis.client = redis.createClient();\\n\\t\\tthis.client.connect();\\n\\t\\treturn new Proxy(this, {\\n\\t\\t\\tget(target, property){\\n\\t\\t\\t\\tconst descriptor = target[property];\\n\\t\\t\\t\\tif(!descriptor){\\n\\t\\t\\t\\t\\treturn;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tif(target.isReady){\\n\\t\\t\\t\\t\\treturn descriptor;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\treturn async function(){\\n\\t\\t\\t\\t\\tawait once(target.client, \'ready\');\\n\\t\\t\\t\\t\\treturn descriptor.apply(target, arguments);\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n        });\\n    }\\n\\n\\tasync add(staffName, reviewId){\\n\\t\\t//Do some business here - idk - like below\\n\\t\\tconst accountName = await this.client.get(staffName);\\n\\t\\treturn this.client.sAdd(`v1:${accountName}:pending-reviews`, reviewId);\\n\\t}\\n}\\n\\nconst client = new ProxiedDistributedDataStructure();\\nclient.add(\'Jerome\', crypto.randomBytes(12).toString(\'hex\'));\\n```\\n\\nThe main benefit for the second approach is that we can instantiate the objects in `sync` contexts and only treat the method calls as `async`  -  instead of needing to play around some dirty gimmicks to call `connect` and chain promises - even worse, callbackifying.\\n\\n**NOTES**: AFAIC from Redis V3^ we have an option `legacyMode` whenever creating the client which we can keep this lazy nature of Redis - doing client buffering of calls."},{"id":"/2024/06/01/Post4","metadata":{"permalink":"/blog/2024/06/01/Post4","source":"@site/blog/2024-06-01/Post4.md","title":"Redis is more than a Cache 2 -> Reference Data","description":"Some weeks ago, we had a incident that was caused mainly due to how we delay job execution.","date":"2024-06-01T00:00:00.000Z","formattedDate":"June 1, 2024","tags":[{"label":"delayed-jobs","permalink":"/blog/tags/delayed-jobs"},{"label":"redis","permalink":"/blog/tags/redis"},{"label":"pubsub","permalink":"/blog/tags/pubsub"},{"label":"distributed-systems","permalink":"/blog/tags/distributed-systems"},{"label":"semaphores","permalink":"/blog/tags/semaphores"},{"label":"async-generators","permalink":"/blog/tags/async-generators"},{"label":"nodejs","permalink":"/blog/tags/nodejs"}],"readingTime":4.86,"hasTruncateMarker":false,"authors":[{"name":"Lucas Weis Polesello","title":"SRE | Senior Software Engineer @ LumaHealth","url":"https://github.com/lukas8219","email":"lucas.polesello@lwpsoftwares.com | lucas.c4d@gmail.com","imageURL":"https://github.com/lukas8219.png","key":"lukas8219"}],"frontMatter":{"title":"Redis is more than a Cache 2 -> Reference Data","authors":"lukas8219","tags":["delayed-jobs","redis","pubsub","distributed-systems","semaphores","async-generators","nodejs"]},"prevItem":{"title":"NodeJS Lazy Initialization","permalink":"/blog/2024/06/01/Post3"}},"content":"Some weeks ago, we had a incident that was caused mainly due to how we delay job execution.\\nOne queue had abnormal behavior during the weekends, which our monitoring systems caught, but we were expecting system to be self-healing and be able to kick through this small hiccup.\\nTuesday we noticed something was off with our integration systems - where a code that was running for more than a year stopped working at the same time we rolled it for a new big customer.\\nI got involved early in the incident due to how often I touch that component of the backend and we initiated investigation so we could get back to the customer ASAP with some good news.\\nAfter some hours of investigation we ended up concluding that our long polling mechanism had a hot-loop in on a certain queue - which was always ending up retrying delays. The first trigger was during the weekends, where we saw the abnormal behavior and it then would always end up in this corner case.\\nDue to always re-delaying jobs, jobs postponed to this queue would never actually drain and thus the handler would eternally get stuck while trying to drain it - delaying all the other queues from the platform.\\nIn a quick 1/2 hour solution we shipped code to stop retrying jobs after more than N retries and consume the hot queue in parallel - to keep processing all jobs in the meantime.\\n\\n### What about the future?\\nAll that to say what? What is has to do with Redis?\\nAs described in this post - the job delaying mechanism lives within Redis where we use an Sorted Set to pull the next job. We fanout to a certain amount in parallel - but we only restart this process after all possible queue targets where drained up until some upper bound score (`score` as score from ZSET - in this case, our timestamp which we delayed the job)\\\\\\nWith this fragility in mind and the upcoming code-freeze of December - I suggested to rewrite this design completely.\\n\\n### Current Design:\\n\\nWhenever a service requests an postpone OR fails to publish some event, wether by not being connected yet (lazy connections), some IO failure(Net/Disk), AMQP Protocol error or even some ephemeral channel churn (close/recreate) - we always pass the job to this `Delayed Jobs` storage.\\n\\nThe current production design is a simple Sorted Set from Redis, where timestamps are the score and the job key itself is the serialized job.\\n\\nCurrently we use a *cache.r7g.12xlarge* ElastiCache (~300GB) sustain our load SPIKES.\\n\\nAnother backend component is then, at each 5s, triggering a long polling mechanism to re-enqueue all jobs until the current time.\\n\\n### New Design\\n\\nI\'ve created some abstractions to make it easier to implement new storage backends like S3, VFS and more.\\nRedis is now running as a reference-to-data component and only stores entire jobs if they are within the next ~2 seconds.\\n\\nStorage backends are decided using some factors like:\\n- How long are we planning to postpone?\\n- How big is this job?\\n- Is this storage backend rolled out for certain queue/tenant?\\n- Is this storage available?\\n\\nAnd they have redundancy - where if one storage fails we use another - falling back in last case for Redis.\\n\\nLets use for example - storing a job for 10minutes that has a medium size due to the context it needs to carry. We would S3 to storage the data.\\nThen we would insert a key into Redis formatted as `s3::some-queue::384192393192::<uuid>`\\n\\nWhen trying to re-enqueue the job - the abstraction easily fetches the protocol `s3` and the identifier `<uuid>` - fetching the S3 file from path `<uuid>`.\\n\\nIn the backend component - we changed some core features from the old service:\\n- Instead of waiting for some schedule job trigger the backend service, it triggers itself given a certain interval. *Removes the need of something triggering it - What if scheduler component dies?*\\n- Process always N queues in parallel using Semaphores to control concurrency.\\n\\t- Removes bottleneck from one slow queue slowing the entire platform.\\n- Limit execution time: We used AbortSignals to prevent long running processing for certain queues and add visibility into long running operations (Prometheus Counter for each timeout/long running).\\n- We keep polling jobs up until a certain timestamp. If the job is passed a due timestamp, we re-insert it to Redis.\\n\\t- This enables us to horizontally scale this component which in the past had to run as singleton due to bad design.\\n\\n### What to expect from this Design:\\n- Infrastructure downsizing:\\n\\t- Instead of storing entire jobs, we store ~128bits of data.\\n\\t- Currently we have 4million of keys stored consuming up to 200GB of RAM\\n\\t- In the new design this would be ~64mb - if we calculate `uuid` being 128 and omit the other small appended data.\\n\\t- Roughly we are expecting to downsize to 10GB - since brief postpones can still occurs and overload it. Experimentation will lead us to decide the proper size.\\n- Independency\\n\\t- Decouple backend components preventing service A stopping service B of working properly\\n- Speed\\n\\t- N replicas will consume queues much faster in some abnormal load.\\n\\t- Maybe S3 storage will be slower - but possibly re-enqueue is a bigger bottleneck than retrieving objects from S3.\\n- Horizontal Scalability/Reliability\\n\\t- If a pod dies, we have redundancy to keep the it running.\\n- Tail Latency Aware\\n\\t- Our latency-sensitive services suffer the most when some issue happens in this component\\n\\t- With this design, latency-sensitive components will be decoupled such as WebSocket notifications, Chat Messages and all real time communication - heavily used in the platform.\\n\\n\\n### Concerns\\nWe are still concerned with RPS, S3 speed for large objects and proper Redis sizing."}]}')}}]);